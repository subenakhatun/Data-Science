{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad01bdf",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "## TF কাকে বলে?\n",
    "#### TF(Term frequency): মানে কোন শব্দ তোমার বাক্যে কতবার এসেছে বা আছে সেটা একটা সংখ্যায় প্রকাশ করা যেন মেশিন এটা বিশ্লেষণ করতে পারে। \n",
    "একটা শব্দ একটা ডকুমেন্ট বা একটা বাক্যে কয়বার আছে। <br>\n",
    "যেমনঃ I am Bangladeshi and i love Bangladesh.<br>\n",
    "এই বাক্যে:<br>\n",
    "I --> 2 বার আছে<br>\n",
    "am ---> ১ বার আছে<br>\n",
    "Bnagladesh ---> ১ বার আছে<br>\n",
    "and ---> ১ বার আছে<br>\n",
    "love ---> ১ বার আছে<br>\n",
    "Bangladeshi ---> ১ বার আছে<br>\n",
    "\n",
    "### TF এর ফর্মুলাঃ \n",
    "TF(word) = (একটা শব্দ কতবার আছে বা এসেছে / মোট শব্দ সংখ্যা) <br>\n",
    "TF('I') = 2/7 = 0.285 <br>\n",
    "TF('love') = 1/7 = 0.142 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d41ebe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequencies (TF):\n",
      "\n",
      "i: 0.286\n",
      "am: 0.143\n",
      "bangladeshi: 0.143\n",
      "and: 0.143\n",
      "love: 0.143\n",
      "bangladesh: 0.143\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Sample sentence\n",
    "text = \"I am Bangladeshi and i love Bangladesh.\"\n",
    "\n",
    "# Step 2: Preprocess - lowercase and tokenize\n",
    "words = text.lower().replace(\".\", \"\").split()\n",
    "\n",
    "# Step 3: Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "total_words = len(words)\n",
    "\n",
    "# Step 4: Calculate TF for each word\n",
    "tf_scores = {word: round(count / total_words, 3) for word, count in word_counts.items()}\n",
    "print(\"Term Frequencies (TF):\\n\")\n",
    "for word, tf in tf_scores.items():\n",
    "    print(f\"{word}: {tf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303549cf",
   "metadata": {},
   "source": [
    "# IDF\n",
    "### IDF কাকে বলেঃ\n",
    "IDF: Inverse Document Frequency <br>\n",
    "একটা শব্দ সব ডকুমেন্টে কতটা কমন। <br>\n",
    "যদি কোন শব্দ অরায় সব বাক্যে থাকে তাহলে সে খুব সাধারণ শব্দ, তার IDF কম হবে।<br>\n",
    "যদি কোন শব্দ একটা বা দুইটা বাক্যে মাত্র ১ বার থাকে সে দুর্ল্ভ বা গুরত্তবপুর্ণ , তার মানে IDF বেশি হবে।<br>\n",
    "যেমনঃ <br>\n",
    "'I love my country'<br>\n",
    "'I love python prgramming.'<br>\n",
    "'I love coding.'<br>\n",
    "৩ টা বাক্যে  i , love আছে, এটা সাধারণ শব্দ যার IDF আসলে কম।<br>\n",
    "\n",
    "##### IDF এর ফর্মুলাঃ \n",
    "IDF(word) = log(মোট ডকুমেন্টের সংখ্যা / যে সব ডকুমেন্টে এই শব্দ আছে)<br>\n",
    "IDF(country) = log(3/1) = 0.477<br>\n",
    "IDF(I) = log(3/3) = 0<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1408ec",
   "metadata": {},
   "source": [
    "## TF*IDF\n",
    "একটা শব্দ একটা ডকুমেন্টে কতটা গুর্তবপুর্ণ। <br>\n",
    "ফর্মুলাঃ TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeed86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ed4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am Bangladeshi and i love Bangladesh.'\n",
    "text1 = 'I know python programming'\n",
    "text2 = 'I love to stay in Bangladesh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1c2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make objects\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8478f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit transform\n",
    "tf_fit = tfidf.fit_transform([text,text1,text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6a19bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0,\n",
       " 'bangladeshi': 3,\n",
       " 'and': 1,\n",
       " 'love': 6,\n",
       " 'bangladesh': 2,\n",
       " 'know': 5,\n",
       " 'python': 8,\n",
       " 'programming': 7,\n",
       " 'to': 10,\n",
       " 'stay': 9,\n",
       " 'in': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check every vocabulary or word in endcoding \n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 13 stored elements and shape (3, 11)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.49047908420610337\n",
      "  (0, 3)\t0.49047908420610337\n",
      "  (0, 1)\t0.49047908420610337\n",
      "  (0, 6)\t0.3730219858594306\n",
      "  (0, 2)\t0.3730219858594306\n",
      "  (1, 5)\t0.5773502691896257\n",
      "  (1, 8)\t0.5773502691896257\n",
      "  (1, 7)\t0.5773502691896257\n",
      "  (2, 6)\t0.3730219858594306\n",
      "  (2, 2)\t0.3730219858594306\n",
      "  (2, 10)\t0.49047908420610337\n",
      "  (2, 9)\t0.49047908420610337\n",
      "  (2, 4)\t0.49047908420610337\n"
     ]
    }
   ],
   "source": [
    "# showing every word importency\n",
    "print(tf_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c4b6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'and', 'bangladesh', 'bangladeshi', 'in', 'know', 'love',\n",
       "       'programming', 'python', 'stay', 'to'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82237c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49047908 0.49047908 0.37302199 0.49047908 0.         0.\n",
      "  0.37302199 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.57735027 0.57735027 0.         0.        ]\n",
      " [0.         0.         0.37302199 0.         0.49047908 0.\n",
      "  0.37302199 0.         0.         0.49047908 0.49047908]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_fit.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e40a42",
   "metadata": {},
   "source": [
    "# Apply Tf-IDF for Email spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc69957",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
